# 根因定位方案设计和开发

## 方案设计

根因定位整体方案如下：

![infer-solution-arch.png](../../images/infer-solution-arch.png)

（说明：灰色部分暂不涉及，会在后续版本考虑进来。）

主要步骤如下：

1. 根据上报的异常KPI事件，触发根因定位流程。
2. 获取异常KPI事件所在时间点的拓扑图。根据异常KPI事件的时间点、异常KPI指标对应的观测实例，查询arangodb，获取以异常观测实例为起点的一个拓扑子图。
3. 基于获取的拓扑图，结合录入的专家规则，构建实时的因果图。
4. 对因果图进行剪枝，删除没有故障的节点。故障节点是指包含异常指标的观测实例。
5. 以异常KPI指标为起点，根据剪枝的因果图进行根因推导，输出top3的根因指标。

### 专家规则

现有专家规则分为两类，分别是：

- 主机内专家规则：定义了单个主机内部的观测实体之间的因果关系。
- 跨主机专家规则：定义了主机之间的观测实体之间的因果关系。

#### 主机内专家规则

主机内专家规则包括：

- 规则1：如果 tcp_link 观测实例 A 和 sli 观测实例 B 属于同一个 process 观测实例 C，则建立 A 到 B 的因果关系。

  进程的 sli 性能观测指标，如请求时延、tps ，往往会受到网络影响。当出现网络故障时，进程的 tcp 连接内相关指标往往会出现异常，进而导致通过该 tcp 连接进行通信的应用层 sli 性能出现劣化。

- 规则2：如果 sli 观测实例 A 到 process 观测实例 B 存在 belongs_to 关系，则建立 B 到 A 的因果关系。

  进程的 sli 性能观测指标会直接受到进程本身运行状态的影响。比如进程的 IO 资源、CPU 资源以及 内存资源等占用过低，都会反映到进程内相关指标上，进而直接导致应用的 sli 性能出现劣化。

- 规则3：如果 block 观测实例 A 到 disk 观测实例 B 存在 belongs_to 关系，则建立 B 到 A 的因果关系。

  一个 IO 读写请求通过文件系统层调用后，再往后会经过通用块层，最终请求到磁盘。当磁盘的 IO 负载过高的时候，会增加 IO 的读写时延，进而影响对应块设备的读写时延。

- 规则4：如果 disk 观测实例 A 和 process 观测实例 B 属于同一个主机，则建立 B 到 A 的因果关系。

  一个 IO 密集型的进程会导致磁盘的 IO 负载过高。

- 规则5：如果 block 观测实例 A 和 process 观测实例 B 属于同一个主机，则建立 A 到 B 的因果关系。

  一个 IO 读写请求通过文件系统层调用后，再往后会经过通用块层，最终请求到磁盘。当块设备的读写时延增大，会导致进程的读写时延增大。

- 规则6：如果观测实例 A 到观测实例 B 存在 runs_on 关系，则建立 B 到 A 的因果关系。

  在物理上具有运行关系的观测实体之间，往往会存在故障传播关系。

- 规则7：如果 cpu 观测实例 A 和 process 观测实例 B 属于同一个主机，则建立 A 到 B 的因果关系。

  当系统 cpu 资源负载比较高的时候，某个进程获得 cpu 调度的时间往往会下降，进而导致进程的性能指标出现劣化。
  
- 规则8：如果 nic 观测实例 A 和 tcp_link 观测实例 B 属于同一个主机，则建立 A 到 B 的因果关系。

  网卡的故障会导致 tcp 连接指标出现异常，比如丢包、时延等。

#### 跨主机专家规则

跨主机专家规则需要针对实际的应用场景进行定制，下面定义了基于 qemu-kvm 的虚拟化场景的专家规则：

- 规则9：在基于 qemu-kvm 的虚拟化场景，如果虚拟机 A 运行在物理机的 qemu 进程 B 上，则建立 A 上 disk 观测实例 C 到 B 的因果关系。

  在该场景下，qemu 进程作为虚拟机的实际 IO 存储后端，当虚拟机 IO 负载冲高时，故障会传播到 qemu 进程。

- 规则10：在基于 qemu-kvm 的虚拟化场景，如果虚拟机 A 运行在物理机的 qemu 进程 B 上，则建立 B 到 A 上 block 观测实例 C 的因果关系。

  在该场景下，qemu 进程作为虚拟机的实际 IO 存储后端，当物理机 IO 性能劣化时，故障会向上传播到虚拟机的 block 层。

针对基于 ceph 的分布式存储场景，专家规则定义如下：

- 规则11：在基于 ceph 的分布式存储场景，如果物理机 A 和物理机 B 存在 store_in 关系，则建立 A 上 qemu 进程观测实例 C 到 B 上 disk 观测实例 D 的因果关系。

  在该场景下，物理机 B 作为物理机 A 上 qemu 进程的实际网络 IO 存储后端，当 qemu 进程的 IO 负载冲高时，可能会导致物理机 B 的磁盘 IO 负载冲高。

- 规则12：在基于 ceph 的分布式存储场景，如果物理机 A 和物理机 B 存在 store_in 关系，则建立 B 上 block 观测实例 C 到 A 上 qemu 进程观测实例 D 的因果关系。

  在该场景下，物理机 B 作为物理机 A 上 qemu 进程的实际网络 IO 存储后端，当物理机 B 的 IO 性能劣化时，故障会向上传播到 qemu 进程。



#### 指标粒度的专家规则

一个观测实体往往包含多种不同类型的观测指标，不同的观测指标之间往往存在不同的因果关系。例如，进程（proc）观测实体包含cpu类、磁盘类、网络类等指标，磁盘（disk）观测实体发生故障时，往往会直接导致进程的磁盘类指标的异常，而不是cpu类、网络类等指标。为了更精准地表达观测实体之间的因果关系，我们在定义了观测实体之间的因果关系的基础之上，添加了指标粒度的因果规则。

首先，我们对观测实体包含的指标集合进行分类。以进程（proc）和 磁盘（disk）为例，它们的指标分类为：

**进程（proc）：**

- **PROC_CPU** ：cpu类指标，指标集合为，
  - gala_gopher_proc_utime_jiffies
  - gala_gopher_proc_stime_jiffies
- **PROC_IO_LOAD** ：磁盘IO负载类指标，指标集合为，
  - gala_gopher_proc_read_bytes
  - gala_gopher_proc_write_bytes
- **PROC_IO_DELAY** ：磁盘IO时延类指标，指标集合为，
  - gala_gopher_proc_iowait_us
  - gala_gopher_proc_bio_latency
- **PROC_NET_DELAY** ：网络时延类指标，指标集合为，
  - gala_gopher_proc_ns_sendmsg
  - gala_gopher_proc_ns_recvmsg

**磁盘（disk）：**

- **DISK_LOAD**：磁盘IO负载类指标，指标集合为，
  - gala_gopher_disk_rspeed_kB
  - gala_gopher_disk_wspeed_kB
  - gala_gopher_disk_rspeed
  - gala_gopher_disk_wspeed
- **DISK_DELAY**：磁盘IO时延类指标，指标集合为，
  - gala_gopher_disk_r_await
  - gala_gopher_disk_w_await
  - gala_gopher_disk_rareq
  - gala_gopher_disk_wareq

其它类型观测实体的指标分类参见配置文件 `config/infer-rule.yaml`。除此之外，我们还定义了3种特殊的指标分类，它们分别是：

- **ALL** ：表示某个观测实体的所有指标分类的集合。比如对于 disk 观测实体，ALL = {DISK_LOAD, DISK_DELAY, OTHER} 。
- **OTHER** ：表示某个观测实体的未被分类的指标集合。比如在 disk 观测实体中，所有不属于 DISK_LOAD 和 DISK_DELAY 的指标都归为 OTHER 类。
- **VIRTUAL_xxx** 类：虚拟指标集合，表示某个观测实体中还未被观测的指标，或者是已经观测但无法有效反映实际变化的指标。现有的虚拟指标分类包括：
  - VIRTUAL ：默认虚拟指标
  - VIRTUAL_IO_DELAY ：IO时延类虚拟指标
  - VIRTUAL_IO_LOAD ：IO负载类虚拟指标
  - VIRTUAL_NET_DELAY ：网络时延类虚拟指标

定义好观测实体的指标分类后，接下来我们就可以定义指标粒度的因果关系了。`规则4`中定义了 proc 到 disk 之间的因果关系，当 proc 正在密集请求磁盘时，它的 IO 负载类指标会冲高，进而导致磁盘排队请求量变大，最终可能导致磁盘请求时延性能劣化。因此我们可以添加如下约束：

```yaml
-
  from_type: proc
  to_type: disk
  metric_range:
  -
    from: PROC_IO_LOAD
    to: ALL
```

该约束表明：对于 proc 到 disk 之间的因果关系，只建立 proc 的 PROC_IO_LOAD 类指标到 disk 的 ALL 类指标（包括DISK_LOAD、DISK_DELAY、OTHER 类）之间的因果关系。如果同一类指标中包含多个异常指标，则会选择根因评分最高的那个指标。因此，在最终构建因果关系时，会建立 proc 中 PROC_IO_LOAD 类指标中根因评分最高的指标到 disk 中每个指标分类中根因评分最高的指标之间的因果关系。例如，gala_gopher_proc_read_bytes -> gala_gopher_disk_rspeed_kB ，gala_gopher_proc_read_bytes -> gala_gopher_disk_r_await 。

详细的指标粒度的专家规则的定义参见配置文件 `config/infer-rule.yaml`。



### 因果图构建

输入：

- 异常 KPI 事件
  - 异常发生时间点
  - 异常 KPI 对应的观测实例
  - 推荐的根因指标
- 基于实体粒度的因果专家规则
- 拓扑图

构建流程如下：

1. 获取拓扑图：根据异常KPI事件的时间点、异常KPI指标对应的观测实例，查询arangodb，获取以异常观测实例为起点的一个拓扑子图，记作 $G^T$。
2. 初始因果图构建：在拓扑图 $G^T$ 上，解析专家规则，将满足条件的观测实例之间建立因果关系，生成观测实例之间的因果图，记作 $G^{IC}$ 。
3. 因果图剪枝：遍历因果图 $G^{IC}$ 中的所有观测实例节点，如果不为故障节点，则从图中删除该节点以及与它相连的邻边。最终生成剪枝后的因果图，记作 $G^{PC}$ 。
4. 生成异常指标之间的因果图：遍历因果图 $G^{PC}$ 中每条观测实例节点之间的因果关系，如果观测实例 A 到观测实例 B 存在一条因果关系，则建立观测实例 A 上所有异常观测指标到观测实例 B 上所有异常观测指标之间的因果关系。最终生成异常指标之间的因果图，记作 $G^{MC}$ ，此时图中的节点表示异常观测指标。

#### 引入系统异常事件

因果图构建的输入中纳入系统异常事件。

实现方案设计：

1. 除了将异常检测推荐的指标作为**可能的根因指标集合**，gala-gopher 产生的系统异常事件的集合并入到**可能的根因指标集合**。
2. 如何评估系统异常事件对KPI的影响得分：将系统异常事件的影响性优先级设置为最高，即影响得分设置为 1 。
3. 如何确定此次根因定位的系统异常事件的集合：以异常KPI发生的时间点为终点，取最近2分钟（可配置）发生的系统异常事件作为此次根因定位的有效集合。
4. 如何获取系统异常事件的集合：创建一个 AbnMetricEvtMgt 类进行管理，每次触发KPI根因定位时会从kafka循环消费系统异常事件并保存，直到系统异常事件发生的时间点超过当前KPI发生的时间点；设置一个老化时间（可配置，比如10分钟），将超过该老化时间的历史系统异常事件从集合中删除；从当前集合中取最近2分钟（可配置）发生的系统异常事件作为此次根因定位的有效集合。

### 根因推导

#### 随机游走算法

根据因果图 $G^{MC}$ ，使用随机游走算法，输出 topK 的根因指标。

![alg-random-walk.png](../../images/alg-random-walk.png)

输入：有向图G，V为服务节点集，E为边集，M为转移概率矩阵，$v_{fe}$为异常的SLI指标。

输出：随机游走结束后每个服务节点经过的次数。

算法步骤：

- 步骤1：以 $v_{fe}$ 作为起点，运行随机游走算法；

- 步骤2：重复 n 轮，每轮：计算当前节点的前向、后向、自向转移概率，基于概率随机选择下一个节点；

- 步骤3：对每个异常指标节点经过的次数排序，选择次数最多的 topK 异常指标节点作为根因。

#### 深度优先搜索算法

根据因果图 $G^{MC}$ ，使用深度优先搜索算法，输出 topK 的根因传播路径，并将传播路径的起始节点作为根因指标。

路径评分：对于给定根因传播路径 $path = {m_1, m_2, ..., m_k}$ ，它的根因得分为，
$$
cause\_score(path) = \{\sum_{i=1}^{k-1}score_{m_i}\} / (k-1).
$$
其中，$score_{m_i}$ 表示指标 $m_i$ 对异常 KPI 的影响性得分（使用 Pearson 相关系数计算）。另外， $m_k$ 即为异常 KPI ，因此不加入根因得分的计算。

### 输入输出

#### 根因定位输出格式

参见[这里](../../guide/zh-CN/api/cause-infer.md)。
